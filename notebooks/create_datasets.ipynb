{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:23.452598Z",
     "start_time": "2020-05-25T20:51:23.168822Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "sys.path.append('../code/')\n",
    "sys.path.append('../python/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:25.032257Z",
     "start_time": "2020-05-25T20:51:23.572732Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from os import path\n",
    "import scipy\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import scipy\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from metrics import ranking\n",
    "# from sh import  sh\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:25.062992Z",
     "start_time": "2020-05-25T20:51:25.033935Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataloader):\n",
    "    x, y = [], []\n",
    "    for batch_x, batch_y in tqdm(iter(dataloader)):\n",
    "        x.append(batch_x.numpy())\n",
    "        y.append(batch_y.numpy())\n",
    "    x = np.vstack(x)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def create_hashgan_train_test(x, y, db_size, query_size):\n",
    "    train_x, query_x, train_y, query_y = train_test_split(x, y, test_size = query_size, stratify = y)\n",
    "    train_x, db_x, train_y, db_y = train_test_split(train_x, train_y, test_size = db_size, stratify = train_y)\n",
    "    \n",
    "    return train_x, train_y, query_x, query_y, db_x, db_y\n",
    "\n",
    "def create_train_test(x, y, query_size):\n",
    "    \"\"\"Train and DB are using the same dataset: gallery\"\"\"\n",
    "    train_x, query_x, train_y, query_y = train_test_split(x, y, test_size = query_size, stratify = y)\n",
    "    \n",
    "    return train_x, train_y, query_x, query_y, train_x, train_y\n",
    "\n",
    "def get_cifar10_data(image_size, batch_size, dataroot='../data/', workers=2, data_transforms=None):\n",
    "    if data_transforms is None:\n",
    "        data_transforms = transforms.Compose([\n",
    "                                transforms.Scale(image_size),\n",
    "                                transforms.ToTensor()\n",
    "                                # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ])\n",
    "    train_dataset = dset.CIFAR10(root=dataroot, download=True, train=True, transform=data_transforms)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers)\n",
    "    test_dataset = dset.CIFAR10(root=dataroot, download=True, train=False, transform=data_transforms)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def get_places365_dataloaders(image_size, batch_size, dataroot, workers=2, data_transforms=None):\n",
    "    if data_transforms is None:\n",
    "        data_transforms = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.ToTensor()\n",
    "                            ])\n",
    "        \n",
    "    train_dataloader = torch.utils.data.DataLoader(dset.ImageFolder(\n",
    "                                                        root=path.join(dataroot, 'train'),\n",
    "                                                        transform=data_transforms\n",
    "                                                    ), \n",
    "                                                   batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "    \n",
    "    valid_dataloader = torch.utils.data.DataLoader(dset.ImageFolder(\n",
    "                                                        root=path.join(dataroot, 'val'),\n",
    "                                                        transform=data_transforms\n",
    "                                                    ), \n",
    "                                                   batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "        \n",
    "    \n",
    "    \n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "def get_mnist_data(image_size, batch_size, dataroot='../data/', workers=2, data_transforms=None):\n",
    "    if data_transforms is None:\n",
    "        data_transforms = transforms.Compose([\n",
    "                                transforms.Scale(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, ), (0.5, )),\n",
    "                            ])\n",
    "    train_dataset = dset.MNIST(root=dataroot, download=True, train=True, transform=data_transforms)\n",
    "    train_x, train_y = get_numpy_data(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers))\n",
    "    test_dataset = dset.MNIST(root=dataroot, download=True, train=False, transform=data_transforms)\n",
    "    test_x, test_y = get_numpy_data(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers))\n",
    "    \n",
    "    x = np.vstack([train_x, test_x])\n",
    "    y = np.concatenate([train_y, test_y])\n",
    "    return x, y\n",
    "\n",
    "def get_mnist_3c_data(image_size, batch_size, dataroot='../data/', workers=2, data_transforms=None):\n",
    "    if data_transforms is None:\n",
    "        data_transforms = transforms.Compose([\n",
    "                                transforms.Scale(image_size),\n",
    "                                transforms.Grayscale(3),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ])\n",
    "    train_dataset = dset.MNIST(root=dataroot, download=True, train=True, transform=data_transforms)\n",
    "    train_x, train_y = get_numpy_data(torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers))\n",
    "    test_dataset = dset.MNIST(root=dataroot, download=True, train=False, transform=data_transforms)\n",
    "    test_x, test_y = get_numpy_data(torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=workers))\n",
    "    \n",
    "    x = np.vstack([train_x, test_x])\n",
    "    y = np.concatenate([train_y, test_y])\n",
    "    return x, y\n",
    "\n",
    "def get_flickr_data(image_size, dataroot='../data/Flickr25K', workers=2, data_transforms=None):\n",
    "    data_transforms = transforms.Compose([\n",
    "                                transforms.Scale(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))])\n",
    "    dataset = torchvision.datasets.ImageFolder(dataroot, transform=data_transforms)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    \n",
    "    test_x, test_y = get_numpy_data(loader)\n",
    "    \n",
    "    x = np.vstack([train_x, test_x])\n",
    "    y = np.concatenate([train_y, test_y])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:25.211376Z",
     "start_time": "2020-05-25T20:51:25.064335Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sample_files_from_list(basedir, file_list, n_per_class, seed, ignored_file_list=set()):\n",
    "    sampled_files = {}\n",
    "    permuted_indices = np.arange(len(file_list))\n",
    "    print('Setting seed {}'.format(seed))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(permuted_indices)\n",
    "    selected_files = []\n",
    "    for idx in tqdm(permuted_indices):\n",
    "        filename = file_list[idx]\n",
    "        if filename not in ignored_file_list:\n",
    "            _, label, img_filename = filename.split('/')\n",
    "            if label not in sampled_files:\n",
    "                sampled_files[label] = []\n",
    "\n",
    "            if len(sampled_files[label]) < n_per_class:\n",
    "                sampled_files[label].append((img_filename, path.join(basedir, filename)))\n",
    "                selected_files.append(filename)\n",
    "    for label, img_list in sampled_files.items():\n",
    "        assert len(img_list) == n_per_class\n",
    "    return sampled_files, selected_files\n",
    "\n",
    "def sample_train_db_data_from_dataloader(dataloader, num_train, num_db, seed):\n",
    "    x, y = get_numpy_data(dataloader)\n",
    "    assert (num_train + num_db) == x.shape[0]\n",
    "    \n",
    "    print('Setting seed {}'.format(seed))\n",
    "    train_x, db_x, train_y, db_y = train_test_split(x, y, train_size = num_train, random_state=seed, stratify = y)\n",
    "    \n",
    "    return train_x, train_y, db_x, db_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:26.325817Z",
     "start_time": "2020-05-25T20:51:26.303229Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_dir_if_not_exist(folder):\n",
    "    if not path.exists(folder):\n",
    "        # print('Creating folder: {}'.format(folder))\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "def create_dataset_from_files(basedir, sampled_files):\n",
    "    if path.exists(basedir):\n",
    "        raise Exception('Directory already exists: {}'.format(basedir))\n",
    "    pbar = tqdm(sampled_files.items())\n",
    "    cnt = 0\n",
    "    try:\n",
    "        for label, img_list in pbar :\n",
    "            label_dir = path.join(basedir, label)\n",
    "            make_dir_if_not_exist(label_dir)\n",
    "\n",
    "            for img_filename, img_path in img_list:\n",
    "                cnt += 1\n",
    "                shutil.copyfile(img_path, path.join(label_dir, img_filename))\n",
    "                if cnt %500 == 0:\n",
    "                    pbar.set_postfix(file_cnt=cnt)\n",
    "        pbar.set_postfix(file_cnt=cnt)\n",
    "    finally:\n",
    "        pbar.close()\n",
    "        \n",
    "def check_evenly_sampling(a):\n",
    "    cnts = np.sum(ranking.one_hot_label(a), axis=0)\n",
    "    for cnt in cnts:\n",
    "        assert cnt == cnts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:51:27.183243Z",
     "start_time": "2020-05-25T20:51:27.166137Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-3C\n",
    "\n",
    "MNIST data with 3 channels (stacking the same copy of the 1-channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:02:18.488856Z",
     "start_time": "2020-05-25T21:01:53.939693Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_x, all_y = get_mnist_3c_data(IMAGE_SIZE, 100, dataroot='../data/', workers=0)\n",
    "dataset = 'mnist-3c'\n",
    "NUM_IMAGES = all_x.shape[0]\n",
    "print('Dataset: {} images'.format(NUM_IMAGES))\n",
    "print('Data range: [{}, {}]'.format(all_x.min(), all_x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:06:25.477466Z",
     "start_time": "2020-05-25T21:02:18.490377Z"
    }
   },
   "outputs": [],
   "source": [
    "# DCW-AE paper\n",
    "for seed, num_query in [\n",
    "        (9, 10000), \n",
    "        (19, 10000), \n",
    "        (29, 10000),\n",
    "        (39, 10000),\n",
    "        (49, 10000)\n",
    "    ]:\n",
    "    num_train = num_db = NUM_IMAGES - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, db_y = train_x, train_y\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:08:04.408531Z",
     "start_time": "2020-05-25T21:06:25.478804Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is used in DistillHash, SSDH papers\n",
    "for seed, num_train, num_query in [\n",
    "        (109, 5000, 10000), \n",
    "        (119, 5000, 10000), \n",
    "        (129, 5000, 10000),\n",
    "        (139, 5000, 10000),\n",
    "        (149, 5000, 10000),\n",
    "    ]:\n",
    "    num_db = NUM_IMAGES - num_train - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, query_x, db_y, query_y = train_test_split(\n",
    "        query_x, query_y, train_size = num_db, random_state=seed, stratify = query_y)\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:54:03.847828Z",
     "start_time": "2020-05-25T20:53:50.194881Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_x, all_y = get_mnist_data(IMAGE_SIZE, 100, dataroot='../data/', workers=0)\n",
    "dataset = 'mnist'\n",
    "NUM_IMAGES = all_x.shape[0]\n",
    "print('Dataset: {} images'.format(NUM_IMAGES))\n",
    "print('Data range: [{}, {}]'.format(all_x.min(), all_x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:56:11.484070Z",
     "start_time": "2020-05-25T20:54:06.082404Z"
    }
   },
   "outputs": [],
   "source": [
    "# DCW-AE paper\n",
    "for seed, num_query in [\n",
    "        (9, 10000), \n",
    "        (19, 10000), \n",
    "        (29, 10000),\n",
    "        (39, 10000),\n",
    "        (49, 10000)\n",
    "    ]:\n",
    "    num_train = num_db = NUM_IMAGES - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, db_y = train_x, train_y\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T04:21:56.914215Z",
     "start_time": "2020-05-19T04:20:38.428436Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# This is used in DistillHash, SSDH papers\n",
    "for seed, num_train, num_query in [\n",
    "        (109, 5000, 10000), \n",
    "        (119, 5000, 10000), \n",
    "        (129, 5000, 10000),\n",
    "        (139, 5000, 10000),\n",
    "        (149, 5000, 10000),\n",
    "    ]:\n",
    "    num_db = NUM_IMAGES - num_train - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, query_x, db_y, query_y = train_test_split(\n",
    "        query_x, query_y, train_size = num_db, random_state=seed, stratify = query_y)\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Flickr25k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T14:34:10.224114Z",
     "start_time": "2020-05-19T14:33:08.126472Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = 'flickr25k'\n",
    "image_size=IMAGE_SIZE\n",
    "dataroot='../data/Flickr25K/'\n",
    "workers=0\n",
    "data_transforms = transforms.Compose([\n",
    "                                transforms.Resize(image_size),\n",
    "                                transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "loader = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(dataroot, transform=data_transforms), \n",
    "                                     batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "all_x, all_y = get_numpy_data(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T14:34:24.744009Z",
     "start_time": "2020-05-19T14:34:24.487778Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES = all_x.shape[0]\n",
    "print('Dataset: {} images'.format(NUM_IMAGES))\n",
    "print('Data range: [{}, {}]'.format(all_x.min(), all_x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T14:42:19.009971Z",
     "start_time": "2020-05-19T14:34:32.172411Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DCW-AE paper\n",
    "for seed, num_query in [\n",
    "        (9, 5000), \n",
    "        (19, 5000), \n",
    "        (29, 5000),\n",
    "        (39, 5000),\n",
    "        (49, 5000)\n",
    "    ]:\n",
    "    num_train = num_db = NUM_IMAGES - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, db_y = train_x, train_y\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T04:52:19.459171Z",
     "start_time": "2020-05-19T04:51:53.294230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "\n",
    "train_dataloader, query_dataloader = get_cifar10_data(IMAGE_SIZE, 100, dataroot='../data/', workers=0)\n",
    "train_x, train_y = get_numpy_data(train_dataloader)\n",
    "query_x, query_y = get_numpy_data(query_dataloader)\n",
    "all_x = np.vstack([train_x, query_x])\n",
    "all_y = np.concatenate([train_y, query_y])\n",
    "NUM_IMAGES = all_x.shape[0]\n",
    "print('Dataset: {} images'.format(NUM_IMAGES))\n",
    "print('Data range: [{}, {}]'.format(all_x.min(), all_x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T05:12:54.524693Z",
     "start_time": "2020-05-19T04:52:19.471348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# DCW-AE paper\n",
    "for seed, num_query in [\n",
    "        (9, 10000), \n",
    "        (19, 10000), \n",
    "        (29, 10000),\n",
    "        (39, 10000),\n",
    "        (49, 10000)\n",
    "    ]:\n",
    "    num_train = num_db = NUM_IMAGES - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, db_y = train_x, train_y\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T05:24:33.987022Z",
     "start_time": "2020-05-19T05:12:54.526133Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is used in DistillHash, SSDH papers\n",
    "for seed, num_train, num_query in [\n",
    "        (109, 5000, 10000), \n",
    "        (119, 5000, 10000), \n",
    "        (129, 5000, 10000),\n",
    "        (139, 5000, 10000),\n",
    "        (149, 5000, 10000),\n",
    "    ]:\n",
    "    num_db = NUM_IMAGES - num_train - num_query\n",
    "    output_dir = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "\n",
    "    print('Setting seed {}: {} train, {} query, {} db'.format(seed, num_train, num_query, num_db))\n",
    "    if path.exists(output_dir):\n",
    "        print('Deleting existing folder: {}'.format(output_dir))\n",
    "        shutil.rmtree(output_dir)\n",
    "    print('Will save in {}'.format(output_dir))\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    \n",
    "    train_x, query_x, train_y, query_y = train_test_split(\n",
    "        all_x, all_y, train_size = num_train, random_state=seed, stratify = all_y)\n",
    "    db_x, query_x, db_y, query_y = train_test_split(\n",
    "        query_x, query_y, train_size = num_db, random_state=seed, stratify = query_y)\n",
    "\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'query')), x = query_x, y=query_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'train')), x = train_x, y=train_y)\n",
    "    np.savez_compressed(path.join(output_dir, '{}_{}_manual_{}.npz'.format(dataset, IMAGE_SIZE, 'db')), x = db_x, y=db_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
