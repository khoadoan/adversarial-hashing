{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:22.737804Z",
     "start_time": "2020-05-25T20:58:22.723637Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python/')\n",
    "sys.path.append('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:22.939351Z",
     "start_time": "2020-05-25T20:58:22.923887Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:24.520891Z",
     "start_time": "2020-05-25T20:58:23.026321Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import data_manual as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:24.531252Z",
     "start_time": "2020-05-25T20:58:24.522423Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:24.684162Z",
     "start_time": "2020-05-25T20:58:24.532701Z"
    }
   },
   "outputs": [],
   "source": [
    "print (torch.__version__)\n",
    "print(torch.cuda.current_device())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:24.879118Z",
     "start_time": "2020-05-25T20:58:24.685594Z"
    }
   },
   "outputs": [],
   "source": [
    "from vgg import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T20:58:36.860120Z",
     "start_time": "2020-05-25T20:58:24.880280Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "vgg16 = VGG16(in_channels=3, output_dim=1000).to(device)\n",
    "# before = vgg_features.state_dict()['fc7.weight']\n",
    "state_dict = load_state_dict_from_url(url=\n",
    "'https://download.pytorch.org/models/vgg16-397923af.pth')\n",
    "\n",
    "vgg16.load_state_dict(state_dict,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:01:32.152973Z",
     "start_time": "2020-05-25T21:01:32.134978Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from os import path\n",
    "\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T21:01:33.674800Z",
     "start_time": "2020-05-25T21:01:33.648606Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_vgg_features(vgg16, loader, num_points, block_idx):\n",
    "    features = np.zeros([num_points, 4096], dtype=np.float32)\n",
    "    labels = []\n",
    "\n",
    "    idx = 0\n",
    "    prev_idx = 0\n",
    "    for batch_x, batch_y in tqdm(iter(loader)):\n",
    "        batch_x = (batch_x + 1) / 2 #Normalize to 0, 1\n",
    "        if idx == 0:\n",
    "            print('Input range: [{}, {}]'.format(batch_x.min().item(), batch_x.max().item()))\n",
    "        #check the batch data, make sure it's in the appropriate range\n",
    "        assert batch_x.min().item() >= 0\n",
    "        assert batch_x.max().item() <= 1\n",
    "        idx = len(batch_x) + prev_idx\n",
    "        features[prev_idx: idx] = vgg16.get_fc7(batch_x.to(device), idx=block_idx).cpu().numpy()\n",
    "        prev_idx = idx\n",
    "        labels += batch_y.detach().cpu().numpy().tolist()\n",
    "    return features, labels\n",
    "\n",
    "def get_vgg_data(dataroot, seed, num_train, num_db, num_query, block_idx):    \n",
    "    train_loader =  data_utils.get_dataloader(dataset, IMAGE_SIZE, 100,\n",
    "                       dataroot=dataroot, workers=0, data_transforms=None, type='train', shuffle=False, clear_cache=True)\n",
    "    db_loader =  data_utils.get_dataloader(dataset, IMAGE_SIZE, 100,\n",
    "                       dataroot=dataroot, workers=0, data_transforms=None, type='db', shuffle=False, clear_cache=True)\n",
    "    query_loader =  data_utils.get_dataloader(dataset, IMAGE_SIZE, 100,\n",
    "                       dataroot=dataroot, workers=0, data_transforms=None, type='query', shuffle=False, clear_cache=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_features, train_labels = get_vgg_features(vgg16, train_loader, num_train, block_idx)\n",
    "        print('train done: data range [{},{}]'.format(train_features.min(), train_features.max()))\n",
    "\n",
    "        query_features, query_labels = get_vgg_features(vgg16, query_loader, num_query, block_idx)\n",
    "        print('query done: data range [{},{}]'.format(train_features.min(), train_features.max()))\n",
    "\n",
    "        db_features, db_labels = get_vgg_features(vgg16, db_loader, num_db, block_idx)\n",
    "        print('db done: data range [{},{}]'.format(db_features.min(), db_features.max()))\n",
    "    return train_features, train_labels, query_features, query_labels, db_features, db_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the created 3-channel mnist dataset in `create_datasets.ipynb`, we can generate the VGG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-25T21:03:56.190Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'mnist-3c'\n",
    "block_idx = 2\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 60000, 60000, 10000),\n",
    "        (19, 60000, 60000, 10000),\n",
    "        (29, 60000, 60000, 10000),\n",
    "        (39, 60000, 60000, 10000),\n",
    "        (49, 60000, 60000, 10000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=block_idx\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T00:27:34.224848Z",
     "start_time": "2020-05-20T00:14:47.388646Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'mnist-3c'\n",
    "block_idx = 1\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 60000, 60000, 10000),\n",
    "        (19, 60000, 60000, 10000),\n",
    "        (29, 60000, 60000, 10000),\n",
    "        (39, 60000, 60000, 10000),\n",
    "        (49, 60000, 60000, 10000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=block_idx\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:03:11.672947Z",
     "start_time": "2020-05-20T00:56:29.502934Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'mnist-3c'\n",
    "block_idx = 1\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (109, 5000, 55000, 10000),\n",
    "        (119, 5000, 55000, 10000),\n",
    "        (129, 5000, 55000, 10000),\n",
    "        (139, 5000, 55000, 10000),\n",
    "        (149, 5000, 55000, 10000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=block_idx\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:07:45.499543Z",
     "start_time": "2020-05-20T01:03:11.674519Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'mnist-3c'\n",
    "block_idx = 2\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (109, 5000, 55000, 10000),\n",
    "        (119, 5000, 55000, 10000),\n",
    "        (129, 5000, 55000, 10000),\n",
    "        (139, 5000, 55000, 10000),\n",
    "        (149, 5000, 55000, 10000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=block_idx\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:35:30.942827Z",
     "start_time": "2020-05-20T01:30:33.124832Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 50000, 50000, 10000),\n",
    "        (19, 50000, 50000, 10000),\n",
    "        (29, 50000, 50000, 10000),\n",
    "        (39, 50000, 50000, 10000),\n",
    "        (49, 50000, 50000, 10000),\n",
    "    ]:    \n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=2\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:46:59.223765Z",
     "start_time": "2020-05-20T01:35:30.946458Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "block_idx = 1\n",
    "\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 50000, 50000, 10000),\n",
    "        (19, 50000, 50000, 10000),\n",
    "        (29, 50000, 50000, 10000),\n",
    "        (39, 50000, 50000, 10000),\n",
    "        (49, 50000, 50000, 10000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=block_idx\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(dataset, block_idx, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLICKR-25K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T00:27:34.442803Z",
     "start_time": "2020-05-20T00:02:24.957Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'flickr25k'\n",
    "\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 20000, 20000, 5000),\n",
    "        (19, 20000, 50000, 5000),\n",
    "        (29, 20000, 20000, 5000),\n",
    "        (39, 20000, 20000, 5000),\n",
    "        (49, 20000, 20000, 5000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=2\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_2_{}_manual_{}.npz'.format(dataset, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T00:27:34.450099Z",
     "start_time": "2020-05-20T00:02:26.351Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'flickr25k'\n",
    "block_idx = 1\n",
    "\n",
    "for (seed, num_train, num_db, num_query) in [\n",
    "        (9, 20000, 20000, 5000),\n",
    "        (19, 20000, 50000, 5000),\n",
    "        (29, 20000, 20000, 5000),\n",
    "        (39, 20000, 20000, 5000),\n",
    "        (49, 20000, 20000, 5000),\n",
    "    ]:\n",
    "    dataroot = '../data/{}_isize{}_seed{}'.format(dataset, IMAGE_SIZE, seed)\n",
    "    train_features, train_labels, query_features, query_labels, db_features, db_labels = get_vgg_data(\n",
    "        dataroot, seed, num_train, num_db, num_query, block_idx=2\n",
    "    )\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(block_idx, dataset, 4096, 'query')), x = query_features, y=query_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(block_idx, dataset, 4096, 'train')), x = train_features, y=train_labels)\n",
    "    np.savez_compressed(path.join(dataroot, '{}_fc7_{}_{}_manual_{}.npz'.format(block_idx, dataset, 4096, 'db')), x = db_features, y=db_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
